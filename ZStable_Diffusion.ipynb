{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JUef3GkOxx-D"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zalring/ZStable-Diffusion/blob/main/ZStable_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stable Diffusion text-to-image and image-to-image synthesis\n",
        "\n",
        "--\n",
        "\n",
        "**NB: As this notebook doesn't rely on Huggingface, for now you'll have to manually [download the model](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original) and then upload it to *'MyDrive/AI/ZStable_Diffusion/models/sd-v1-4.ckpt'* (by default on Colab) in order to use it.**\n",
        "\n",
        "--\n",
        "\n",
        "#### From the [Stable Diffusion](https://github.com/CompVis/stable-diffusion) repo :\n",
        "\n",
        "- Stable Diffusion is a latent text-to-image diffusion model. Thanks to a generous compute donation from Stability AI and support from LAION, we were able to train a Latent Diffusion Model on 512x512 images from a subset of the LAION-5B database. Similar to Google's Imagen, this model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM. See this section below and the model card.\n",
        "- Stable Diffusion v1 refers to a specific configuration of the model architecture that uses a downsampling-factor 8 autoencoder with an 860M UNet and CLIP ViT-L/14 text encoder for the diffusion model. The model was pretrained on 256x256 images and then finetuned on 512x512 images.\n",
        "- By using a diffusion-denoising mechanism as first proposed by SDEdit, the model can be used for different tasks such as text-guided image-to-image translation and upscaling. Similar to the txt2img sampling script, we provide a script to perform image modification with Stable Diffusion.\n",
        "\n",
        "--\n",
        "\n",
        "Special features of this Colab :\n",
        "- Settings saving\n",
        "- Better image save management\n",
        "- Multi-prompts (1 per iteration)\n",
        "- Make easier to use your txt2img/img2img outputs as img2img inputs (multiple inputs for img2img possible)\n",
        "- [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN) upscaling and face enhancement\n",
        "- No NSFW filter\n",
        "- txt2img->img2img and img2img->img2img recursive generation through prompts pipes and weights\n",
        "- parameter for GFPGAN face enhancement of intermediate results in case of recursive generation\n",
        "- Module for a deep cleaning of the VRAM allowing to solve some cases of CUDA error requiring a full restart until then\n",
        "- **NEW!** Optionally, run [Optimized SD (+turbo mode)](https://github.com/basujindal/stable-diffusion), which uses much less VRAM\n",
        "- **NEW!** Create [seamless tileable textures](https://www.reddit.com/r/StableDiffusion/comments/x5q7no/native_seamless_tile_generation_no_inpainting/)\n",
        "\n",
        "--\n",
        "\n",
        "Colab assembled by [Zalring](https://twitter.com/ZalringTW)."
      ],
      "metadata": {
        "id": "NUmmV5ZvrPbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Check GPU"
      ],
      "metadata": {
        "id": "ThxmCePqt1mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check GPU\n",
        "\n",
        "import subprocess\n",
        "for o in subprocess.run(\"nvidia-smi\", stdout=subprocess.PIPE).stdout.decode('utf-8').split('\\n') : print(o)\n",
        "#!nvidia-smi"
      ],
      "metadata": {
        "id": "jbL2zJ7Pt7Jl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup options"
      ],
      "metadata": {
        "id": "I9-UBqjntIoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Enable saving outputs to Google Drive to save your creations at AI/models\n",
        "save_outputs_to_google_drive = True #@param {type:\"boolean\"}\n",
        "#@markdown Enable saving models to Google Drive to avoid downloading the model every Colab instance\n",
        "save_models_to_google_drive = True #@param {type:\"boolean\"}\n",
        "#@markdown Enable the load of an upscaler\n",
        "load_upscaler = True #@param {type:\"boolean\"}\n",
        "#@markdown Override default path for the models if filled\n",
        "override_models_path = \"\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VKHfzYZgtNWM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "JUef3GkOxx-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "3gR1V-knxd40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, os, sys, ipykernel, shutil\n",
        "\n",
        "def gitclone(url):\n",
        "  res = subprocess.run(['git', 'clone', url], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "  print(res)\n",
        "\n",
        "def pipi(modulestr):\n",
        "  res = subprocess.run(['pip', 'install', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "  print(res)\n",
        "\n",
        "def pipir(txtfile):\n",
        "  res = subprocess.run(['pip', 'install', 'r', txtfile], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "  print(res)\n",
        "\n",
        "def pipie(modulestr):\n",
        "  res = subprocess.run(['git', 'install', '-e', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "  print(res)\n",
        "\n",
        "def wget(url, outputdir):\n",
        "  res = subprocess.run(['wget', url, '-P', f'{outputdir}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "  print(res)\n",
        "\n",
        "def createPath(filepath):\n",
        "  os.makedirs(filepath, exist_ok=True)"
      ],
      "metadata": {
        "id": "76fHqCVsxfjZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save model and outputs on Google Drive if available"
      ],
      "metadata": {
        "id": "uWLsDt7wkZfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if save_outputs_to_google_drive or save_models_to_google_drive:\n",
        "  try : # Colab\n",
        "      from google.colab import drive\n",
        "      use_colab = True\n",
        "      try:\n",
        "        drive.mount('/content/drive')\n",
        "        root_path = \"/content/drive/MyDrive/AI/ZStable_Diffusion\"\n",
        "      except:\n",
        "        print(\"Google Colab not detected.\")\n",
        "        #save_outputs_to_google_drive = False\n",
        "        #save_models_to_google_drive = False\n",
        "        root_path = \"/content\"\n",
        "  except : # Local\n",
        "    root_path = os.getcwd()\n",
        "    # If running locally, there's a good chance your env will need this in order to not crash upon np.matmul() or similar operations.\n",
        "    os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
        "    use_colab = False\n",
        "\n",
        "model_path = f'{root_path}/models' if override_models_path.strip() == '' else override_models_path.strip()\n",
        "outputs_path = f'{root_path}/images_out'\n",
        "\n",
        "createPath(model_path) #!mkdir -p $model_path\n",
        "createPath(outputs_path)#!mkdir -p $outputs_path\n",
        "\n",
        "print(f\"Model will be stored at {model_path}\")\n",
        "print(f\"Outputs will be saved to {outputs_path}\")\n"
      ],
      "metadata": {
        "id": "aJF6wP2zkWE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e79b2658-ed75-4ee8-dcbf-e9e1e80ca63f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Model will be stored at /content/drive/MyDrive/AI/ZStable_Diffusion/models\n",
            "Outputs will be saved to /content/drive/MyDrive/AI/ZStable_Diffusion/images_out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download code and model"
      ],
      "metadata": {
        "id": "xEVSOJ4f0B21"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NHgUAp48qwoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99122caa-65ea-459b-ba55-86097e62ce0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting omegaconf>=2.0.0\n",
            "  Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from omegaconf>=2.0.0) (6.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
            "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=5b6e6c3a7746befa96704f5a500473264b23d8455c583c71dee2040b50d8dd26\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/8d/53/2af8772d9aec614e3fc65e53d4a993ad73c61daa8bbd85a873\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.2.3\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning>=1.0.8\n",
            "  Downloading pytorch_lightning-1.7.5-py3-none-any.whl (706 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.8) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.8) (4.1.1)\n",
            "Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.8) (1.12.1+cu113)\n",
            "Collecting tensorboard>=2.9.1\n",
            "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.8) (4.64.0)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.8) (1.21.6)\n",
            "Collecting pyDeprecate>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.8) (21.3)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.8) (2022.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8) (2.23.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8) (3.8.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8) (1.3.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8) (2.1.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8) (22.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8) (1.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning>=1.0.8) (3.0.9)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (57.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (1.47.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (0.37.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (3.4.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (1.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning>=1.0.8) (3.2.0)\n",
            "Installing collected packages: torchmetrics, tensorboard, pyDeprecate, pytorch-lightning\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "Successfully installed pyDeprecate-0.3.2 pytorch-lightning-1.7.5 tensorboard-2.10.0 torchmetrics-0.9.3\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.3\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kornia\n",
            "  Downloading kornia-0.6.7-py2.py3-none-any.whl (565 kB)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from kornia) (1.12.1+cu113)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kornia) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->kornia) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kornia) (3.0.9)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.6.7\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Stable Diffusion\n",
        "\n",
        "sys.path.append(\".\")\n",
        "\n",
        "gitclone(\"https://github.com/CompVis/taming-transformers\")\n",
        "pipie(\"./taming-transformers\")\n",
        "sys.path.append('./taming-transformers')\n",
        "\n",
        "pipi(\"omegaconf>=2.0.0\")\n",
        "pipi(\"pytorch-lightning>=1.0.8\")\n",
        "pipi(\"einops\")\n",
        "pipi(\"transformers\")\n",
        "pipi(\"kornia\")\n",
        "pipi(\"ftfy\")\n",
        "\n",
        "gitclone(\"https://github.com/openai/CLIP.git\")\n",
        "pipie(\"./CLIP\")\n",
        "sys.path.append('./CLIP')\n",
        "\n",
        "gitclone(\"https://github.com/basujindal/stable-diffusion.git\")\n",
        "#gitclone(\"https://github.com/CompVis/stable-diffusion.git\")\n",
        "sys.path.append('./stable-diffusion')\n",
        "sys.path.append('./stable-diffusion/optimizedSD')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Real-ESRGAN\n",
        "if load_upscaler:\n",
        "  gitclone(\"https://github.com/xinntao/Real-ESRGAN.git\")\n",
        "  #sys.path.append('./Real-ESRGAN')\n",
        "\n",
        "  os.chdir(\"./Real-ESRGAN\")\n",
        "  print(\"Change working dir to: \"+os.getcwd())\n",
        "  #%cd /content/Real-ESRGAN\n",
        "\n",
        "  pipi(\"basicsr\")\n",
        "  pipi(\"facexlib\")\n",
        "  pipi(\"gfpgan\")\n",
        "  pipir(\"./Real-ESRGAN/requirements.txt\")\n",
        "\n",
        "  subprocess.run(['python', 'setup.py', 'develop'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "  #!python setup.py develop\n",
        "  os.chdir(\"../\")\n",
        "  print(\"Change working dir to: \"+os.getcwd())\n",
        "  #%cd /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZJk19BTkdqZ",
        "outputId": "ad3d44e7-b493-4756-8e00-13eefd5dd270"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Change working dir to: /content/Real-ESRGAN\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting basicsr\n",
            "  Downloading basicsr-1.4.2.tar.gz (172 kB)\n",
            "Collecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from basicsr) (0.16.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from basicsr) (0.99)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from basicsr) (1.21.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from basicsr) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from basicsr) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from basicsr) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from basicsr) (2.23.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from basicsr) (0.18.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from basicsr) (1.7.3)\n",
            "Collecting tb-nightly\n",
            "  Downloading tb_nightly-2.11.0a20220908-py3-none-any.whl (5.9 MB)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from basicsr) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from basicsr) (0.13.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from basicsr) (4.64.0)\n",
            "Collecting yapf\n",
            "  Downloading yapf-0.32.0-py2.py3-none-any.whl (190 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->basicsr) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr) (1.24.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr) (2.9.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr) (2021.11.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr) (2.6.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->basicsr) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->basicsr) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->basicsr) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->basicsr) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->basicsr) (1.15.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (1.47.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (3.4.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (1.2.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->basicsr) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly->basicsr) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly->basicsr) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly->basicsr) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->basicsr) (3.2.0)\n",
            "Building wheels for collected packages: basicsr\n",
            "  Building wheel for basicsr (setup.py): started\n",
            "  Building wheel for basicsr (setup.py): finished with status 'done'\n",
            "  Created wheel for basicsr: filename=basicsr-1.4.2-py3-none-any.whl size=214840 sha256=643080df12e08add32751e70ff1ce17a4d804ec7116f20b46c075673afbc2b85\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/b3/4a/e2bc545f98417b6766ca50dd82b2a1f2b37780c68d41da9ca9\n",
            "Successfully built basicsr\n",
            "Installing collected packages: yapf, tb-nightly, addict, basicsr\n",
            "Successfully installed addict-2.4.0 basicsr-1.4.2 tb-nightly-2.11.0a20220908 yapf-0.32.0\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting facexlib\n",
            "  Downloading facexlib-0.2.5-py3-none-any.whl (59 kB)\n",
            "Collecting filterpy\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from facexlib) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facexlib) (1.21.6)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from facexlib) (0.56.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from facexlib) (7.1.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facexlib) (0.13.1+cu113)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from facexlib) (4.6.0.66)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from facexlib) (4.64.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from facexlib) (1.12.1+cu113)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from filterpy->facexlib) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->filterpy->facexlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->filterpy->facexlib) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->facexlib) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba->facexlib) (4.12.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba->facexlib) (0.39.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->facexlib) (3.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->facexlib) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->facexlib) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->facexlib) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->facexlib) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->facexlib) (1.24.3)\n",
            "Building wheels for collected packages: filterpy\n",
            "  Building wheel for filterpy (setup.py): started\n",
            "  Building wheel for filterpy (setup.py): finished with status 'done'\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110474 sha256=3478aa270e3f1c600bd0b5da5ada4dc2cfdd2653a1ff91a89c75627903882763\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/e0/ee/a2b3c5caab3418c1ccd8c4de573d4cbe13315d7e8b0a55fbc2\n",
            "Successfully built filterpy\n",
            "Installing collected packages: filterpy, facexlib\n",
            "Successfully installed facexlib-0.2.5 filterpy-1.4.5\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gfpgan\n",
            "  Downloading gfpgan-1.3.5-py3-none-any.whl (47 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gfpgan) (1.21.6)\n",
            "Requirement already satisfied: tb-nightly in /usr/local/lib/python3.7/dist-packages (from gfpgan) (2.11.0a20220908)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gfpgan) (4.6.0.66)\n",
            "Requirement already satisfied: facexlib>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from gfpgan) (0.2.5)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.7/dist-packages (from gfpgan) (0.32.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from gfpgan) (0.13.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gfpgan) (4.64.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from gfpgan) (6.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from gfpgan) (0.99)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from gfpgan) (1.12.1+cu113)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gfpgan) (1.7.3)\n",
            "Requirement already satisfied: basicsr>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from gfpgan) (1.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->gfpgan) (2.23.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->gfpgan) (0.18.3)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->gfpgan) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->gfpgan) (7.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.4.2->gfpgan) (0.16.0)\n",
            "Requirement already satisfied: filterpy in /usr/local/lib/python3.7/dist-packages (from facexlib>=0.2.5->gfpgan) (1.4.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from facexlib>=0.2.5->gfpgan) (0.56.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->gfpgan) (4.1.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from filterpy->facexlib>=0.2.5->gfpgan) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->gfpgan) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->gfpgan) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->gfpgan) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->gfpgan) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->filterpy->facexlib>=0.2.5->gfpgan) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba->facexlib>=0.2.5->gfpgan) (4.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->facexlib>=0.2.5->gfpgan) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba->facexlib>=0.2.5->gfpgan) (0.39.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->facexlib>=0.2.5->gfpgan) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr>=1.4.2->gfpgan) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr>=1.4.2->gfpgan) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr>=1.4.2->gfpgan) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr>=1.4.2->gfpgan) (3.0.4)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr>=1.4.2->gfpgan) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr>=1.4.2->gfpgan) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr>=1.4.2->gfpgan) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr>=1.4.2->gfpgan) (2.9.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (1.2.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (1.8.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (1.47.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->gfpgan) (3.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->gfpgan) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->gfpgan) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->gfpgan) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->gfpgan) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly->gfpgan) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->gfpgan) (3.2.0)\n",
            "Installing collected packages: gfpgan\n",
            "Successfully installed gfpgan-1.3.5\n",
            "\n",
            "\n",
            "Change working dir to: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Download the models"
      ],
      "metadata": {
        "id": "fNqCqQDoyZmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SD\n",
        "if os.path.isfile(f\"{model_path}/sd-v1-4.ckpt\"):\n",
        "    print(\"Stable diffusion: Using saved model from Google Drive\")\n",
        "else:\n",
        "    print(\"For now, you need to provide your own source for the model.\")\n",
        "    #wget(\"IDK !\", model_path)"
      ],
      "metadata": {
        "id": "cNHvQBhzyXCI",
        "outputId": "d7a7dec8-3d47-4587-8688-7ff8c124a611",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stable diffusion: Using saved model from Google Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Real-ESRGAN\n",
        "if load_upscaler: \n",
        "  if os.path.isfile(f\"{model_path}/RealESRGAN_x4plus.pth\"):\n",
        "      print(\"Real-ESRGAN: Using saved model from Google Drive\")\n",
        "  else:\n",
        "      wget('https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth', model_path)\n",
        "      #!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n",
        "\n",
        "  if os.path.isfile(f\"{model_path}/detection_Resnet50_Final.pth\"):\n",
        "      print(\"Real-ESRGAN: Using saved model from Google Drive\")\n",
        "  else:\n",
        "      wget('https://github.com/xinntao/facexlib/releases/download/v0.1.0/detection_Resnet50_Final.pth', model_path)\n",
        "\n",
        "  if os.path.isfile(f\"{model_path}/parsing_parsenet.pth\"):\n",
        "      print(\"Real-ESRGAN: Using saved model from Google Drive\")\n",
        "  else:\n",
        "      wget('https://github.com/xinntao/facexlib/releases/download/v0.2.2/parsing_parsenet.pth', model_path)\n",
        "\n",
        "  if os.path.isfile(f\"{model_path}/GFPGANv1.3.pth\"):\n",
        "      print(\"Real-ESRGAN: Using saved model from Google Drive\")\n",
        "  else:\n",
        "      wget('https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth', model_path)\n",
        "\n",
        "  shutil.copy(f\"{model_path}/RealESRGAN_x4plus.pth\", './Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus.pth')\n",
        "\n",
        "  os.makedirs('/content/Real-ESRGAN/gfpgan/weights/', exist_ok=True)\n",
        "  shutil.copy(f\"{model_path}/detection_Resnet50_Final.pth\", '/content/Real-ESRGAN/gfpgan/weights/detection_Resnet50_Final.pth')\n",
        "  shutil.copy(f\"{model_path}/parsing_parsenet.pth\", '/content/Real-ESRGAN/gfpgan/weights/parsing_parsenet.pth')\n",
        "  #shutil.copy(f\"{model_path}/detection_Resnet50_Final.pth\", '/usr/local/lib/python3.7/dist-packages/facexlib/weights/detection_Resnet50_Final.pth')\n",
        "  #shutil.copy(f\"{model_path}/parsing_parsenet.pth\", '/usr/local/lib/python3.7/dist-packages/facexlib/weights/parsing_parsenet.pth')\n",
        "  shutil.copy(f\"{model_path}/GFPGANv1.3.pth\", '/usr/local/lib/python3.7/dist-packages/gfpgan/weights/GFPGANv1.3.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8-wbbHco7bD",
        "outputId": "984fef4e-0272-435e-865c-bc924f1fa71a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real-ESRGAN: Using saved model from Google Drive\n",
            "Real-ESRGAN: Using saved model from Google Drive\n",
            "Real-ESRGAN: Using saved model from Google Drive\n",
            "Real-ESRGAN: Using saved model from Google Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "5fO_vegZ5_Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse, os, sys, glob\n",
        "from taming.models import vqgan\n",
        "import torch\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange, repeat\n",
        "from torchvision.utils import make_grid\n",
        "import time\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch import autocast\n",
        "from contextlib import contextmanager, nullcontext\n",
        "\n",
        "import gc\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "#%cd /content/stable-diffusion\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "#%cd /content"
      ],
      "metadata": {
        "id": "BPnyd-XUKbfE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "uIRA4x7Y6Z95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load general functions"
      ],
      "metadata": {
        "id": "LjZcDMYPm3iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reach(li, to_n):\n",
        "    if len(li)==0:\n",
        "        return li\n",
        "    return (li + ([li[-1]] * max(0, to_n-len(li))))[:to_n]\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "def load_model(ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    return sd\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def load_img(path, w, h, img_src=None):\n",
        "    if img_src is None :\n",
        "      image = Image.open(path).convert(\"RGB\")\n",
        "      print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
        "    else :\n",
        "      image = img_src.convert(\"RGB\")\n",
        "      print(f\"loaded input image from previous generation\")\n",
        "    #w, h = image.size\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1."
      ],
      "metadata": {
        "id": "l8_r-DwNm72s"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load SD functions"
      ],
      "metadata": {
        "id": "3Vc1AzCwnNc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def txt2img(opt):\n",
        "    \"\"\"\n",
        "    opt :\n",
        "      prompt (str) : the prompt to render\n",
        "      outdir (str) : dir to write results to\n",
        "      skip_grid (bool) : do not save a grid, only individual samples. Helpful when evaluating lots of samples\n",
        "      skip_save (bool) : do not save individual samples. For speed measurements\n",
        "      ddim_steps (int, default=50) : number of ddim sampling steps\n",
        "      plms (bool) : use plms sampling instead of ddim\n",
        "      #   laion400m (bool) : uses the LAION400M model\n",
        "      fixed_code (bool) : if enabled, uses the same starting code across samples\n",
        "      ddim_eta (float, default=0.0) : ddim eta (eta=0.0 corresponds to deterministic sampling)\n",
        "      n_iter (int, default=2) : sample this often\n",
        "      H (int, default=512) : image height, in pixel space\n",
        "      W (int, default=512) : image width, in pixel space\n",
        "      C (int, default=4) : latent channels\n",
        "      f (int, default=8) : downsampling factor\n",
        "      n_samples (int, default=3) : how many samples to produce for each given prompt. A.k.a. batch size\n",
        "      n_rows (int, default=0) : rows in the grid (default: n_samples)\n",
        "      scale (float, default=7.5) : unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n",
        "      #   from_file (str) : if specified, load prompts from this file\n",
        "      #   config (str, default=\"configs/stable-diffusion/v1-inference.yaml\") : path to config which constructs model\n",
        "      #   ckpt (str, default=\"models/ldm/stable-diffusion-v1/model.ckpt\") : path to checkpoint of model\n",
        "      seed (int) : the seed (for reproducible sampling)\n",
        "      precision (str, choices=[\"full\", \"autocast\"], default=\"autocast\") : evaluate at this precision\n",
        "    \"\"\"\n",
        "\n",
        "    #if opt.laion400m:\n",
        "    #    print(\"Falling back to LAION 400M model...\")\n",
        "    #    opt.config = \"configs/latent-diffusion/txt2img-1p4B-eval.yaml\"\n",
        "    #    opt.ckpt = \"models/ldm/text2img-large/model.ckpt\"\n",
        "    #    opt.outdir = \"outputs/txt2img-samples-laion400m\"\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    seed_everything(opt.seed)\n",
        "\n",
        "    if opt.sampler == 'plms':\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "\n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "    outpath = opt.outdir\n",
        "\n",
        "    batch_size = opt.n_samples\n",
        "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
        "\n",
        "    sample_path = os.path.join(outpath, \"samples\")\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "    grid_count = len(os.listdir(outpath)) - 1\n",
        "\n",
        "    base_count = 0\n",
        "    for d in glob.glob(sample_path+\"/*\"):\n",
        "        try :\n",
        "            bname = int(os.path.basename(d)[:-4])\n",
        "            if bname >= base_count:\n",
        "                base_count = bname+1\n",
        "        except : pass\n",
        "    #base_count = len(os.listdir(sample_path))\n",
        "    bbase_count = base_count\n",
        "\n",
        "\n",
        "    start_code = None\n",
        "    if opt.fixed_code:\n",
        "        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
        "\n",
        "    precision_scope = autocast if opt.precision==\"autocast\" else nullcontext\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                tic = time.time()\n",
        "                all_samples = list()\n",
        "                for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
        "\n",
        "                    prompt = opt.prompt[n].split(\"|\")\n",
        "                    recurse = len(prompt[1:])\n",
        "                    sub_prompt = prompt[0].split(\":\")[0].strip()\n",
        "                    try : face_enhance = True if (load_upscaler and prompt[0].split(\":\")[1].strip()[-1] == 'f') else False\n",
        "                    except : face_enhance = False\n",
        "\n",
        "                    data = [batch_size * [sub_prompt]]\n",
        "\n",
        "                    #else:\n",
        "                    #    print(f\"reading prompts from {opt.from_file}\")\n",
        "                    #    with open(opt.from_file, \"r\") as f:\n",
        "                    #        data = f.read().splitlines()\n",
        "                    #        data = list(chunk(data, batch_size))\n",
        "\n",
        "                    for prompts in tqdm(data, desc=\"data\"):\n",
        "                        uc = None\n",
        "                        if opt.scale != 1.0:\n",
        "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "                        c = model.get_learned_conditioning(prompts)\n",
        "                        shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                        samples_ddim, _ = sampler.sample(S=opt.ddim_steps,\n",
        "                                                         conditioning=c,\n",
        "                                                         batch_size=opt.n_samples,\n",
        "                                                         shape=shape,\n",
        "                                                         verbose=False,\n",
        "                                                         unconditional_guidance_scale=opt.scale,\n",
        "                                                         unconditional_conditioning=uc,\n",
        "                                                         eta=opt.ddim_eta,\n",
        "                                                         x_T=start_code)\n",
        "\n",
        "                        x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "                        x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                        if recurse > 0 :\n",
        "                            opt2 = deepcopy(opt)\n",
        "                            opt2.prompt = [\"|\".join(prompt[1:])]\n",
        "                            opt2.n_iter = 1\n",
        "                            opt2.n_samples = 1\n",
        "                            opt2.n_rows = 0\n",
        "                            opt2.seed = hash(str(opt2.seed))%(2**32)\n",
        "                            try :\n",
        "                              subprompt_params = prompt[1].split(\":\")[1].strip()\n",
        "                              if subprompt_params[-1] == 'f' : subprompt_params = subprompt_params[:-1]\n",
        "                              opt2.sub_strength = float(subprompt_params)\n",
        "                            except : opt2.sub_strength = None\n",
        "\n",
        "                            \n",
        "                        for i in range(len(x_samples_ddim)):\n",
        "                            x_sample_t = x_samples_ddim[i]\n",
        "                            x_sample = 255. * rearrange(x_sample_t.cpu().numpy(), 'c h w -> h w c')\n",
        "                            res_img = Image.fromarray(x_sample.astype(np.uint8))\n",
        "                            if recurse > 0 :\n",
        "                                if face_enhance :\n",
        "                                    res_img = upscale(\"\", \"sdres.png\", 1.0, 1.0, True, res_img, True)\n",
        "                                opt2.recurse_img = res_img\n",
        "                                x_samples_ddim[i] = img2img(opt2)\n",
        "                            elif not opt.skip_save :\n",
        "                                if face_enhance :\n",
        "                                    res_img = upscale(\"\", \"sdres.png\", 1.0, 1.0, True, res_img, True)\n",
        "                                res_img.save(os.path.join(sample_path, f\"{base_count}.png\"))\n",
        "                                base_count += 1\n",
        "\n",
        "                        if not opt.skip_grid:\n",
        "                            all_samples.append(x_samples_ddim)\n",
        "\n",
        "                if not opt.skip_grid:\n",
        "                    # additionally, save as grid\n",
        "                    grid = torch.stack(all_samples, 0)\n",
        "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "                    grid = make_grid(grid, nrow=n_rows)\n",
        "\n",
        "                    # to image\n",
        "                    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "                    gimg = Image.fromarray(grid.astype(np.uint8))\n",
        "                    gimg.save(os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
        "                    grid_count += 1\n",
        "\n",
        "                    display(gimg) # TMP\n",
        "\n",
        "                toc = time.time()\n",
        "\n",
        "    print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
        "          f\" \\nEnjoy.\")\n",
        "    \n",
        "    return bbase_count\n",
        "\n",
        "def img2img(opt):\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    seed_everything(opt.seed)\n",
        "\n",
        "    if opt.sampler == 'plms':\n",
        "        raise NotImplementedError(\"PLMS sampler not (yet) supported\")\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "\n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "    outpath = opt.outdir\n",
        "\n",
        "    batch_size = opt.n_samples\n",
        "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
        "\n",
        "    sample_path = os.path.join(outpath, \"samples\")\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "    grid_count = len(os.listdir(outpath)) - 1\n",
        "\n",
        "    base_count = 0\n",
        "    for d in glob.glob(sample_path+\"/*\"):\n",
        "        try :\n",
        "            bname = int(os.path.basename(d)[:-4])\n",
        "            if bname >= base_count:\n",
        "                base_count = bname+1\n",
        "        except : pass\n",
        "    #base_count = len(os.listdir(sample_path))\n",
        "    bbase_count = base_count\n",
        "\n",
        "    init_latent = []\n",
        "    for n in range(opt.n_iter) :\n",
        "        #assert os.path.isfile(opt.init_img[n])\n",
        "        init_image = load_img((opt.init_img[n] if opt.recurse_img is None else None), opt.W, opt.H, opt.recurse_img).to(device)\n",
        "        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "        init_latent.append(model.get_first_stage_encoding(model.encode_first_stage(init_image)))  # move to latent space\n",
        "\n",
        "    sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
        "\n",
        "    strength = (opt.strength if opt.sub_strength is None else opt.sub_strength)\n",
        "    assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
        "    t_enc = int(strength * opt.ddim_steps)\n",
        "    print(f\"target t_enc is {t_enc} steps\")\n",
        "\n",
        "    precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                tic = time.time()\n",
        "                all_samples = list()\n",
        "                for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
        "\n",
        "                    prompt = opt.prompt[n].split(\"|\")\n",
        "                    recurse = len(prompt[1:])\n",
        "                    sub_prompt = prompt[0].split(\":\")[0].strip()\n",
        "                    try : face_enhance = True if (load_upscaler and prompt[0].split(\":\")[1].strip()[-1] == 'f') else False\n",
        "                    except : face_enhance = False\n",
        "\n",
        "                    data = [batch_size * [sub_prompt]]\n",
        "                    #else:\n",
        "                    #    print(f\"reading prompts from {opt.from_file}\")\n",
        "                    #    with open(opt.from_file, \"r\") as f:\n",
        "                    #        data = f.read().splitlines()\n",
        "                    #        data = list(chunk(data, batch_size))\n",
        "\n",
        "                    for prompts in tqdm(data, desc=\"data\"):\n",
        "                        uc = None\n",
        "                        if opt.scale != 1.0:\n",
        "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "                        c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                        # encode (scaled latent)\n",
        "                        z_enc = sampler.stochastic_encode(init_latent[n], torch.tensor([t_enc]*batch_size).to(device))\n",
        "                        # decode it\n",
        "                        samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
        "                                                 unconditional_conditioning=uc,)\n",
        "\n",
        "                        x_samples = model.decode_first_stage(samples)\n",
        "                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                        if recurse > 0 :\n",
        "                            opt2 = deepcopy(opt)\n",
        "                            opt2.prompt = [\"|\".join(prompt[1:])]\n",
        "                            opt2.n_iter = 1\n",
        "                            opt2.n_samples = 1\n",
        "                            opt2.n_rows = 0\n",
        "                            opt2.seed = hash(str(opt2.seed))%(2**32)\n",
        "                            try :\n",
        "                              subprompt_params = prompt[1].split(\":\")[1].strip()\n",
        "                              if subprompt_params[-1] == 'f' : subprompt_params = subprompt_params[:-1]\n",
        "                              opt2.sub_strength = float(subprompt_params)\n",
        "                            except : opt2.sub_strength = None\n",
        "\n",
        "                        for x_sample_t in x_samples:\n",
        "                            x_sample = 255. * rearrange(x_sample_t.cpu().numpy(), 'c h w -> h w c')\n",
        "                            res_img = Image.fromarray(x_sample.astype(np.uint8))\n",
        "                            if recurse > 0 :\n",
        "                                if face_enhance :\n",
        "                                    res_img = upscale(\"\", \"sdres.png\", 1.0, 1.0, True, res_img, True)\n",
        "                                opt2.recurse_img = res_img\n",
        "                                x_sample_t = img2img(opt2)\n",
        "                            elif not opt.skip_save :\n",
        "                                if face_enhance :\n",
        "                                    res_img = upscale(\"\", \"sdres.png\", 1.0, 1.0, True, res_img, True)\n",
        "                                res_img.save(os.path.join(sample_path, f\"{base_count}.png\"))\n",
        "                                base_count += 1\n",
        "                            if opt.recurse_img is not None : # only 1 result in recursive mode\n",
        "                                return x_sample_t\n",
        "\n",
        "                        if not opt.skip_grid:\n",
        "                            all_samples.append(x_samples)\n",
        "\n",
        "                if not opt.skip_grid:\n",
        "                    # additionally, save as grid\n",
        "                    grid = torch.stack(all_samples, 0)\n",
        "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "                    grid = make_grid(grid, nrow=n_rows)\n",
        "\n",
        "                    # to image\n",
        "                    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "                    gimg = Image.fromarray(grid.astype(np.uint8))\n",
        "                    gimg.save(os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
        "                    grid_count += 1\n",
        "\n",
        "                    display(gimg) # TMP\n",
        "\n",
        "                toc = time.time()\n",
        "\n",
        "    print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
        "          f\" \\nEnjoy.\")\n",
        "\n",
        "    return bbase_count"
      ],
      "metadata": {
        "id": "WJRLfLzhncF-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load SD functions (optimized)"
      ],
      "metadata": {
        "id": "WnTZYeElla6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd /content/stable-diffusion\n",
        "\n",
        "def opt_txt2img(opt):\n",
        "    \"\"\"\n",
        "    opt :\n",
        "      prompt (str) : the prompt to render\n",
        "      outdir (str) : dir to write results to\n",
        "      skip_grid (bool) : do not save a grid, only individual samples. Helpful when evaluating lots of samples\n",
        "      skip_save (bool) : do not save individual samples. For speed measurements\n",
        "      ddim_steps (int, default=50) : number of ddim sampling steps\n",
        "      plms (bool) : use plms sampling instead of ddim\n",
        "      #   laion400m (bool) : uses the LAION400M model\n",
        "      fixed_code (bool) : if enabled, uses the same starting code across samples\n",
        "      ddim_eta (float, default=0.0) : ddim eta (eta=0.0 corresponds to deterministic sampling)\n",
        "      n_iter (int, default=2) : sample this often\n",
        "      H (int, default=512) : image height, in pixel space\n",
        "      W (int, default=512) : image width, in pixel space\n",
        "      C (int, default=4) : latent channels\n",
        "      f (int, default=8) : downsampling factor\n",
        "      n_samples (int, default=3) : how many samples to produce for each given prompt. A.k.a. batch size\n",
        "      n_rows (int, default=0) : rows in the grid (default: n_samples)\n",
        "      scale (float, default=7.5) : unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\n",
        "      #   from_file (str) : if specified, load prompts from this file\n",
        "      #   config (str, default=\"configs/stable-diffusion/v1-inference.yaml\") : path to config which constructs model\n",
        "      #   ckpt (str, default=\"models/ldm/stable-diffusion-v1/model.ckpt\") : path to checkpoint of model\n",
        "      seed (int) : the seed (for reproducible sampling)\n",
        "      precision (str, choices=[\"full\", \"autocast\"], default=\"autocast\") : evaluate at this precision\n",
        "    \"\"\"\n",
        "\n",
        "    #if opt.laion400m:\n",
        "    #    print(\"Falling back to LAION 400M model...\")\n",
        "    #    opt.config = \"configs/latent-diffusion/txt2img-1p4B-eval.yaml\"\n",
        "    #    opt.ckpt = \"models/ldm/text2img-large/model.ckpt\"\n",
        "    #    opt.outdir = \"outputs/txt2img-samples-laion400m\"\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    seed_everything(opt.seed)\n",
        "\n",
        "    #if opt.plms:\n",
        "    #    sampler = PLMSSampler(model)\n",
        "    #else:\n",
        "    #    sampler = DDIMSampler(model)\n",
        "\n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "    outpath = opt.outdir\n",
        "\n",
        "    batch_size = opt.n_samples\n",
        "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
        "\n",
        "    sample_path = os.path.join(outpath, \"samples\")\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "    grid_count = len(os.listdir(outpath)) - 1\n",
        "\n",
        "    base_count = 0\n",
        "    for d in glob.glob(sample_path+\"/*\"):\n",
        "        try :\n",
        "            bname = int(os.path.basename(d)[:-4])\n",
        "            if bname >= base_count:\n",
        "                base_count = bname+1\n",
        "        except : pass\n",
        "    #base_count = len(os.listdir(sample_path))\n",
        "    bbase_count = base_count\n",
        "\n",
        "\n",
        "    start_code = None\n",
        "    if opt.fixed_code:\n",
        "        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
        "\n",
        "    \n",
        "    if opt.precision == \"autocast\" and device != \"cpu\":\n",
        "        precision_scope = autocast\n",
        "    else:\n",
        "        precision_scope = nullcontext\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            #with model.ema_scope():\n",
        "                tic = time.time()\n",
        "                all_samples = list()\n",
        "                for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
        "\n",
        "                    prompt = opt.prompt[n].split(\"|\")\n",
        "                    recurse = len(prompt[1:])\n",
        "                    sub_prompt = prompt[0].split(\":\")[0].strip()\n",
        "                    try : face_enhance = True if (load_upscaler and prompt[0].split(\":\")[1].strip()[-1] == 'f') else False\n",
        "                    except : face_enhance = False\n",
        "\n",
        "                    data = [batch_size * [sub_prompt]]\n",
        "\n",
        "                    #else:\n",
        "                    #    print(f\"reading prompts from {opt.from_file}\")\n",
        "                    #    with open(opt.from_file, \"r\") as f:\n",
        "                    #        data = f.read().splitlines()\n",
        "                    #        data = list(chunk(data, batch_size))\n",
        "\n",
        "                    for prompts in tqdm(data, desc=\"data\"):\n",
        "\n",
        "                        modelCS.to(device)\n",
        "                        uc = None\n",
        "\n",
        "                        if opt.scale != 1.0:\n",
        "                            uc = modelCS.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "                        c = modelCS.get_learned_conditioning(prompts)\n",
        "\n",
        "                        shape = [opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "\n",
        "                        if device != \"cpu\":\n",
        "                            mem = torch.cuda.memory_allocated() / 1e6\n",
        "                            modelCS.to(\"cpu\")\n",
        "                            while torch.cuda.memory_allocated() / 1e6 >= mem:\n",
        "                                time.sleep(1)\n",
        "\n",
        "                        samples_ddim = model.sample(\n",
        "                            S=opt.ddim_steps,\n",
        "                            conditioning=c,\n",
        "                            seed=opt.seed,\n",
        "                            shape=shape,\n",
        "                            verbose=False,\n",
        "                            unconditional_guidance_scale=opt.scale,\n",
        "                            unconditional_conditioning=uc,\n",
        "                            eta=opt.ddim_eta,\n",
        "                            x_T=start_code,\n",
        "                            sampler = opt.sampler,\n",
        "                        )\n",
        "\n",
        "                        modelFS.to(device)\n",
        "\n",
        "                        x_samples_ddim = modelFS.decode_first_stage(samples_ddim)\n",
        "                        x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                        if device != \"cpu\":\n",
        "                            mem = torch.cuda.memory_allocated() / 1e6\n",
        "                            modelFS.to(\"cpu\")\n",
        "                            while torch.cuda.memory_allocated() / 1e6 >= mem:\n",
        "                                time.sleep(1)\n",
        "\n",
        "                        if recurse > 0 :\n",
        "                            opt2 = deepcopy(opt)\n",
        "                            opt2.prompt = [\"|\".join(prompt[1:])]\n",
        "                            opt2.n_iter = 1\n",
        "                            opt2.n_samples = 1\n",
        "                            opt2.n_rows = 0\n",
        "                            opt2.seed = hash(str(opt2.seed))%(2**32)\n",
        "                            try :\n",
        "                              subprompt_params = prompt[1].split(\":\")[1].strip()\n",
        "                              if subprompt_params[-1] == 'f' : subprompt_params = subprompt_params[:-1]\n",
        "                              opt2.sub_strength = float(subprompt_params)\n",
        "                            except : opt2.sub_strength = None\n",
        "\n",
        "                            \n",
        "                        for i in range(len(x_samples_ddim)):\n",
        "                            x_sample_t = x_samples_ddim[i]\n",
        "                            x_sample = 255. * rearrange(x_sample_t.cpu().numpy(), 'c h w -> h w c')\n",
        "                            res_img = Image.fromarray(x_sample.astype(np.uint8))\n",
        "                            if recurse > 0 :\n",
        "                                if face_enhance :\n",
        "                                    res_img = upscale(\"\", \"sdres.png\", 1.0, 1.0, True, res_img, True)\n",
        "                                opt2.recurse_img = res_img\n",
        "                                x_samples_ddim[i] = opt_img2img(opt2)\n",
        "                            elif not opt.skip_save :\n",
        "                                if face_enhance :\n",
        "                                    res_img = upscale(\"\", \"sdres.png\", 1.0, 1.0, True, res_img, True)\n",
        "                                res_img.save(os.path.join(sample_path, f\"{base_count}.png\"))\n",
        "                                base_count += 1\n",
        "                            opt.seed += 1\n",
        "                        \n",
        "                        if not opt.skip_grid:\n",
        "                            all_samples.append(x_samples_ddim)\n",
        "\n",
        "                if not opt.skip_grid:\n",
        "                    # additionally, save as grid\n",
        "                    grid = torch.stack(all_samples, 0)\n",
        "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "                    grid = make_grid(grid, nrow=n_rows)\n",
        "\n",
        "                    # to image\n",
        "                    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "                    gimg = Image.fromarray(grid.astype(np.uint8))\n",
        "                    gimg.save(os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
        "                    grid_count += 1\n",
        "\n",
        "                    display(gimg) # TMP\n",
        "\n",
        "                toc = time.time()\n",
        "\n",
        "    print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
        "          f\" \\nEnjoy.\")\n",
        "    \n",
        "    return bbase_count\n",
        "\n",
        "def opt_img2img(opt):\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    seed_everything(opt.seed)\n",
        "\n",
        "    #if opt.plms:\n",
        "    #    raise NotImplementedError(\"PLMS sampler not (yet) supported\")\n",
        "    #    sampler = PLMSSampler(model)\n",
        "    #else:\n",
        "    #    sampler = DDIMSampler(model)\n",
        "\n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "    outpath = opt.outdir\n",
        "\n",
        "    batch_size = opt.n_samples\n",
        "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
        "\n",
        "    sample_path = os.path.join(outpath, \"samples\")\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "    grid_count = len(os.listdir(outpath)) - 1\n",
        "\n",
        "    base_count = 0\n",
        "    for d in glob.glob(sample_path+\"/*\"):\n",
        "        try :\n",
        "            bname = int(os.path.basename(d)[:-4])\n",
        "            if bname >= base_count:\n",
        "                base_count = bname+1\n",
        "        except : pass\n",
        "    #base_count = len(os.listdir(sample_path))\n",
        "    bbase_count = base_count\n",
        "\n",
        "    modelFS.to(device)\n",
        "\n",
        "    init_latent = []\n",
        "    for n in range(opt.n_iter) :\n",
        "        #assert os.path.isfile(opt.init_img[n])\n",
        "        init_image = load_img((opt.init_img[n] if opt.recurse_img is None else None), opt.W, opt.H, opt.recurse_img).to(device)\n",
        "        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "\n",
        "        if opt.precision == \"autocast\" and device != \"cpu\":\n",
        "            init_image = init_image.half()\n",
        "            \n",
        "        init_latent.append(modelFS.get_first_stage_encoding(modelFS.encode_first_stage(init_image)))  # move to latent space\n",
        "    \n",
        "    if device != \"cpu\":\n",
        "        mem = torch.cuda.memory_allocated() / 1e6\n",
        "        modelFS.to(\"cpu\")\n",
        "        while torch.cuda.memory_allocated() / 1e6 >= mem:\n",
        "            time.sleep(1)\n",
        "    #sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
        "\n",
        "    strength = (opt.strength if opt.sub_strength is None else opt.sub_strength)\n",
        "    assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
        "    t_enc = int(strength * opt.ddim_steps)\n",
        "    print(f\"target t_enc is {t_enc} steps\")\n",
        "\n",
        "    if opt.precision == \"autocast\" and device != \"cpu\":\n",
        "        precision_scope = autocast\n",
        "    else:\n",
        "        precision_scope = nullcontext\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            #with model.ema_scope():\n",
        "                tic = time.time()\n",
        "                all_samples = list()\n",
        "                for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
        "\n",
        "                    prompt = opt.prompt[n].split(\"|\")\n",
        "                    recurse = len(prompt[1:])\n",
        "                    sub_prompt = prompt[0].split(\":\")[0].strip()\n",
        "                    try : face_enhance = True if (load_upscaler and prompt[0].split(\":\")[1].strip()[-1] == 'f') else False\n",
        "                    except : face_enhance = False\n",
        "\n",
        "                    data = [batch_size * [sub_prompt]]\n",
        "                    #else:\n",
        "                    #    print(f\"reading prompts from {opt.from_file}\")\n",
        "                    #    with open(opt.from_file, \"r\") as f:\n",
        "                    #        data = f.read().splitlines()\n",
        "                    #        data = list(chunk(data, batch_size))\n",
        "\n",
        "                    for prompts in tqdm(data, desc=\"data\"):\n",
        "\n",
        "                        modelCS.to(device)\n",
        "                        uc = None\n",
        "\n",
        "                        if opt.scale != 1.0:\n",
        "                            uc = modelCS.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "                        c = modelCS.get_learned_conditioning(prompts)\n",
        "\n",
        "                        if device != \"cpu\":\n",
        "                            mem = torch.cuda.memory_allocated() / 1e6\n",
        "                            modelCS.to(\"cpu\")\n",
        "                            while torch.cuda.memory_allocated() / 1e6 >= mem:\n",
        "                                time.sleep(1)\n",
        "\n",
        "                        # encode (scaled latent)\n",
        "                        z_enc = model.stochastic_encode(\n",
        "                            init_latent[n],\n",
        "                            torch.tensor([t_enc] * batch_size).to(device),\n",
        "                            opt.seed,\n",
        "                            opt.ddim_eta,\n",
        "                            opt.ddim_steps,\n",
        "                        )\n",
        "                        # decode it\n",
        "                        samples = model.sample(\n",
        "                            t_enc,\n",
        "                            c,\n",
        "                            z_enc,\n",
        "                            unconditional_guidance_scale=opt.scale,\n",
        "                            unconditional_conditioning=uc,\n",
        "                            sampler = opt.sampler\n",
        "                        )\n",
        "\n",
        "                        modelFS.to(device)\n",
        "\n",
        "                        x_samples = modelFS.decode_first_stage(samples)\n",
        "                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                        if device != \"cpu\":\n",
        "                            mem = torch.cuda.memory_allocated() / 1e6\n",
        "                            modelFS.to(\"cpu\")\n",
        "                            while torch.cuda.memory_allocated() / 1e6 >= mem:\n",
        "                                time.sleep(1)\n",
        "\n",
        "                        if recurse > 0 :\n",
        "                            opt2 = deepcopy(opt)\n",
        "                            opt2.prompt = [\"|\".join(prompt[1:])]\n",
        "                            opt2.n_iter = 1\n",
        "                            opt2.n_samples = 1\n",
        "                            opt2.n_rows = 0\n",
        "                            opt2.seed = hash(str(opt2.seed))%(2**32)\n",
        "                            try :\n",
        "                              subprompt_params = prompt[1].split(\":\")[1].strip()\n",
        "                              if subprompt_params[-1] == 'f' : subprompt_params = subprompt_params[:-1]\n",
        "                              opt2.sub_strength = float(subprompt_params)\n",
        "                            except : opt2.sub_strength = None\n",
        "\n",
        "                        for x_sample_t in x_samples:\n",
        "                            x_sample = 255. * rearrange(x_sample_t.cpu().numpy(), 'c h w -> h w c')\n",
        "                            res_img = Image.fromarray(x_sample.astype(np.uint8))\n",
        "                            if recurse > 0 :\n",
        "                                if face_enhance :\n",
        "                                    res_img = upscale(\"\", \"sdres.png\", 1.0, 1.0, True, res_img, True)\n",
        "                                opt2.recurse_img = res_img\n",
        "                                x_sample_t = opt_img2img(opt2)\n",
        "                            elif not opt.skip_save :\n",
        "                                if face_enhance :\n",
        "                                    res_img = upscale(\"\", \"sdres.png\", 1.0, 1.0, True, res_img, True)\n",
        "                                res_img.save(os.path.join(sample_path, f\"{base_count}.png\"))\n",
        "                                base_count += 1\n",
        "                            opt.seed += 1\n",
        "                            if opt.recurse_img is not None : # only 1 result in recursive mode\n",
        "                                return x_sample_t\n",
        "\n",
        "                        if not opt.skip_grid:\n",
        "                            all_samples.append(x_samples)\n",
        "\n",
        "                if not opt.skip_grid:\n",
        "                    # additionally, save as grid\n",
        "                    grid = torch.stack(all_samples, 0)\n",
        "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "                    grid = make_grid(grid, nrow=n_rows)\n",
        "\n",
        "                    # to image\n",
        "                    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "                    gimg = Image.fromarray(grid.astype(np.uint8))\n",
        "                    gimg.save(os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
        "                    grid_count += 1\n",
        "\n",
        "                    display(gimg) # TMP\n",
        "\n",
        "                toc = time.time()\n",
        "\n",
        "    print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
        "          f\" \\nEnjoy.\")\n",
        "\n",
        "    return bbase_count\n",
        "\n",
        "\n",
        "#%cd /content"
      ],
      "metadata": {
        "id": "HY_7vvnPThzS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load upscaler functions"
      ],
      "metadata": {
        "id": "RTwoSYDnlhZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if load_upscaler :\n",
        "  def upscale(upload_folderr, filename, upscale_factor, downscale_factor, face_enhance, input_img=None, return_img=False):\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    os.chdir(\"./Real-ESRGAN\")\n",
        "    print(\"Change working dir to: \"+os.getcwd())\n",
        "    #%cd ./Real-ESRGAN\n",
        "\n",
        "    upload_folder = './upload/'\n",
        "    result_folder = './results/'\n",
        "\n",
        "    if os.path.isdir(upload_folder):\n",
        "        shutil.rmtree(upload_folder)\n",
        "    if os.path.isdir(result_folder):\n",
        "        shutil.rmtree(result_folder)\n",
        "    os.mkdir(upload_folder)\n",
        "    os.mkdir(result_folder)\n",
        "\n",
        "    dst_path = os.path.join(upload_folder, filename)\n",
        "    input_path = os.path.join(upload_folderr, filename)\n",
        "    \n",
        "    if input_img is None :\n",
        "      shutil.copy(input_path, dst_path)\n",
        "    else :\n",
        "      input_img.save(dst_path)\n",
        "\n",
        "    size = len(filename)\n",
        "\n",
        "    def_upscale_factor = 1.0\n",
        "    \n",
        "    # if it is out of memory, try to use the `--tile` option\n",
        "    if face_enhance :\n",
        "      # more efficient if splited in two parts\n",
        "      subprocess.run(['python', 'inference_realesrgan.py', '-n', 'RealESRGAN_x4plus', '-i', 'upload', '--outscale', str(def_upscale_factor), '--face_enhance'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "      #!python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale $def_upscale_factor --face_enhance\n",
        "      if def_upscale_factor != upscale_factor :\n",
        "        os.remove(dst_path)\n",
        "        shutil.move(f'{result_folder}{filename[:size - 4]}_out.png', dst_path)\n",
        "        subprocess.run(['python', 'inference_realesrgan.py', '-n', 'RealESRGAN_x4plus', '-i', 'upload', '--outscale', str(upscale_factor)], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        #!python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale $upscale_factor\n",
        "    else :\n",
        "      subprocess.run(['python', 'inference_realesrgan.py', '-n', 'RealESRGAN_x4plus', '-i', 'upload', '--outscale', str(upscale_factor)], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "      #!python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale $upscale_factor\n",
        "    # Arguments\n",
        "    # -n, --model_name: Model names\n",
        "    # -i, --input: input folder or image\n",
        "    # --outscale: Output scale, can be arbitrary scale factor.\n",
        "\n",
        "    new_filename = f'{filename[:size - 4]}_out.png'\n",
        "    computed_file_path = f'{result_folder}{new_filename}'\n",
        "\n",
        "    if return_img :\n",
        "      image = Image.open(computed_file_path).convert(\"RGB\")\n",
        "\n",
        "      os.chdir(\"../\")\n",
        "      print(\"Change working dir to: \"+os.getcwd())\n",
        "      #%cd ..\n",
        "\n",
        "      if downscale_factor != 1.0 :\n",
        "        w, h = image.size\n",
        "        w = int(w/downscale_factor)\n",
        "        h = int(h/downscale_factor)\n",
        "        image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "\n",
        "      return image\n",
        "      \n",
        "    else :\n",
        "      new_new_filename = f'{filename[:size - 4]}_UPSCALED.png'\n",
        "      destination_file_path = f'{upload_folderr}/{new_new_filename}'\n",
        "\n",
        "      shutil.move(computed_file_path, destination_file_path)\n",
        "\n",
        "      os.chdir(\"../\")\n",
        "      print(\"Change working dir to: \"+os.getcwd())\n",
        "      #%cd ..\n",
        "\n",
        "      if downscale_factor != 1.0 :\n",
        "        image = Image.open(destination_file_path).convert(\"RGB\")\n",
        "        w, h = image.size\n",
        "        w = int(w/downscale_factor)\n",
        "        h = int(h/downscale_factor)\n",
        "        image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "        image.save(destination_file_path)\n",
        "\n",
        "      return new_new_filename\n"
      ],
      "metadata": {
        "id": "vDvj13b1kRym"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load SD Model"
      ],
      "metadata": {
        "id": "hi4nkWnGcW5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd /content/stable-diffusion\n",
        "\n",
        "#@markdown Use an optimized version of Stable Diffusion (less used VRAM but slower)\n",
        "use_optim_sd = True #@param {type:\"boolean\"}\n",
        "#@markdown Use turbo mode for the optimized version of Stable Diffusion (slightly more used VRAM than for the default optimized version but faster)\n",
        "use_optim_sd_turbo = True #@param {type:\"boolean\"}\n",
        "precision = \"autocast\" #@param [\"autocast\", \"full\"] {type:\"string\"}\n",
        "sd_model_name = \"sd-v1-4.ckpt\" #@param{type:\"string\"}\n",
        "#@markdown Beware of visual horrors if enabled while generating faces...\n",
        "seamless_tileable_img = False #@param {type:\"boolean\"}\n",
        "\n",
        "try:\n",
        "    base_conv_init\n",
        "except :\n",
        "    base_conv_init = torch.nn.Conv2d.__init__\n",
        "\n",
        "def patch_conv(**patch):\n",
        "    init = base_conv_init\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        return init(self, *args, **kwargs, **patch)\n",
        "    torch.nn.Conv2d.__init__ = __init__\n",
        "\n",
        "if seamless_tileable_img :\n",
        "    patch_conv(padding_mode='circular')\n",
        "else :\n",
        "    patch_conv(padding_mode='zeros')\n",
        "\n",
        "if use_optim_sd :\n",
        "    config = \"./stable-diffusion/optimizedSD/v1-inference.yaml\"\n",
        "    ckpt = f\"{model_path}/{sd_model_name}\"\n",
        "\n",
        "    sd = load_model(f\"{ckpt}\")\n",
        "    li, lo = [], []\n",
        "    for key, value in sd.items():\n",
        "        sp = key.split(\".\")\n",
        "        if (sp[0]) == \"model\":\n",
        "            if \"input_blocks\" in sp:\n",
        "                li.append(key)\n",
        "            elif \"middle_block\" in sp:\n",
        "                li.append(key)\n",
        "            elif \"time_embed\" in sp:\n",
        "                li.append(key)\n",
        "            else:\n",
        "                lo.append(key)\n",
        "    for key in li:\n",
        "        sd[\"model1.\" + key[6:]] = sd.pop(key)\n",
        "    for key in lo:\n",
        "        sd[\"model2.\" + key[6:]] = sd.pop(key)\n",
        "\n",
        "    config = OmegaConf.load(f\"{config}\")\n",
        "\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    model = instantiate_from_config(config.modelUNet)\n",
        "    _, _ = model.load_state_dict(sd, strict=False)\n",
        "    model.eval()\n",
        "    model.unet_bs = 1\n",
        "    model.cdevice = device\n",
        "\n",
        "    if use_optim_sd_turbo :\n",
        "      model.turbo = True\n",
        "    else :\n",
        "      model.turbo = False\n",
        "\n",
        "    modelCS = instantiate_from_config(config.modelCondStage)\n",
        "    _, _ = modelCS.load_state_dict(sd, strict=False)\n",
        "    modelCS.eval()\n",
        "    modelCS.cond_stage_model.device = device\n",
        "\n",
        "    modelFS = instantiate_from_config(config.modelFirstStage)\n",
        "    _, _ = modelFS.load_state_dict(sd, strict=False)\n",
        "    modelFS.eval()\n",
        "    del sd\n",
        "\n",
        "    if device != \"cpu\" and precision == \"autocast\":\n",
        "        model.half()\n",
        "        modelCS.half()\n",
        "        modelFS.half()\n",
        "\n",
        "else:\n",
        "    config = OmegaConf.load(\"./stable-diffusion/configs/stable-diffusion/v1-inference.yaml\")\n",
        "    model = load_model_from_config(config, f\"{model_path}/{sd_model_name}\")\n",
        "\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "#%cd /content"
      ],
      "metadata": {
        "id": "tnnslChScVjQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Params tuto"
      ],
      "metadata": {
        "id": "MnCEFPQbK4gO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **prompt (list(str) or str) :** The prompt to render. It can be a list of prompts, each one being tied to one iteration.\n",
        "- **init_img (list(str) or str or list(int) or int) :** **ONLY FOR img2img.** Path to the input image. It can be a list of paths, each image being tied to one iteration. NB: each image will be resized to HxW. If you type integer numbers instead of string paths, the process will use the corresponding generated images of the current folder (defined by **outdir**).\n",
        "\n",
        "- **use_img2img (bool) :** Use an init image.\n",
        "- **outdir (str) :** Directory to write results to.\n",
        "- **skip_grid (bool) :** Do not save a grid, only individual samples. Helpful when evaluating lots of samples.\n",
        "- **strength (float, default=0.75) :** **ONLY FOR img2img.** strength for noising/unnoising. 1.0 corresponds to full destruction of information in init image\n",
        "- **ddim_steps (int, default=50) :** Number of DDIM sampling steps.\n",
        "- **plms (bool) :** Use PLMS sampling instead of DDIM.\n",
        "- **ddim_eta (float, default=0.0) :** DDIM eta (eta=0.0 corresponds to deterministic sampling).\n",
        "- **n_iter (int, default=2) :** Sample this often.\n",
        "- **n_samples (int, default=3) :** How many samples to produce for each given prompt. A.k.a. batch size.\n",
        "- **H (int, default=512) :** Image height, in pixel space.\n",
        "- **W (int, default=512) :** Image width, in pixel space.\n",
        "- **f (int, default=8) :** Downsampling factor.\n",
        "- **scale (float, default=7.5) :** Unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty)).-\n",
        "- **seed (int) :** The seed (for reproducible sampling). Random if set to -1.\n"
      ],
      "metadata": {
        "id": "JCnHKy21MOh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts and init images"
      ],
      "metadata": {
        "id": "I3fekPPOPG_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through pipes (\"|\") and weights from 0.0 to 1.0 (eg. \":0.5\" which correspond to the strength parameter for this subprompt) you can divide your prompts to automatically run again SD on a generated image to fine tune your result. Useful to make sure that heavy prompts are fully taken into account. Also, you can decide to face enhance your intermediate results by adding \"f\" after the weights.\n",
        "\n",
        "**Example** : \"My subprompt 1 decribing a very detailed portrait :f | My subprompt 2 describing a very detailed style :0.5f | My subprompt 3 just because I need one more example :0.75\". "
      ],
      "metadata": {
        "id": "St1zgrCyQYQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = [\n",
        "#\"Portrait of Margaret Thatcher as the Joker, cinematic lighting, highly detailed, digital painting, artstation, illustration, art by greg rutkowski and alphonse mucha.\",\n",
        "\"Portrait of Margaret Thatcher as the Joker, cinematic lighting, highly detailed, digital painting, artstation, illustration, art by greg rutkowski and alphonse mucha. :f | Portrait of Ronald Reagan as the devil, cinematic lighting, highly detailed, digital painting, artstation, illustration, art by greg rutkowski and alphonse mucha. :0.5\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "qdVTZlcw5ajf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "init_img = [] # [\"./drive/MyDrive/XXX.png\"]"
      ],
      "metadata": {
        "id": "PkNeXYglItGC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run!"
      ],
      "metadata": {
        "id": "Z-J8eQDa0Kam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parameters\n",
        "\n",
        "import random as rd\n",
        "import json\n",
        "\n",
        "use_img2img = False #@param {type:\"boolean\"}\n",
        "outdir = \"TimeToStableDiff\" #@param{type:\"string\"}\n",
        "skip_grid = False #@param {type:\"boolean\"}\n",
        "#skip_save = False #@param {type:\"boolean\"}\n",
        "strength = 0.5 #@param {type:\"number\"}\n",
        "ddim_steps = 50 #@param{type:\"integer\"}\n",
        "sampler = \"ddim\" #@param [\"ddim\", \"plms\"] {type:\"string\"}\n",
        "#fixed_code = False #@param {type:\"boolean\"}\n",
        "ddim_eta = 0.5 #@param {type:\"number\"}\n",
        "n_iter = 1 #@param{type:\"integer\"}\n",
        "n_samples = 3 #@param{type:\"integer\"}\n",
        "H = 512 #@param{type:\"integer\"}\n",
        "W = 512 #@param{type:\"integer\"}\n",
        "f = 8\n",
        "scale = 7.5 #@param {type:\"number\"}\n",
        "seed = -1 #@param{type:\"integer\"}\n",
        "\n",
        "abs_outdir = f'{outputs_path}/{outdir}'\n",
        "createPath(abs_outdir)\n",
        "true_seed = (rd.randint(0, (2**32)-1) if (seed==-1) else seed)\n",
        "\n",
        "if not isinstance(init_img, list) : init_img_li = [init_img]\n",
        "else : init_img_li = init_img.copy()\n",
        "if use_img2img :\n",
        "  abs_n_iter = len(init_img_li)\n",
        "else :\n",
        "  abs_n_iter = n_iter\n",
        "  init_img_li = []\n",
        "\n",
        "if not isinstance(prompt, list) : prompt_li = [prompt]\n",
        "else : prompt_li = prompt.copy()\n",
        "prompt_li = reach(prompt_li, abs_n_iter)\n",
        "\n",
        "abs_init_img = []\n",
        "for p in init_img_li :\n",
        "  if isinstance(p, int) :\n",
        "    abs_init_img.append(f\"{abs_outdir}/samples/{p}.png\")\n",
        "  else : abs_init_img.append(p)\n",
        "\n",
        "args = argparse.Namespace(\n",
        "  prompt = prompt_li,\n",
        "  init_img = abs_init_img,\n",
        "  outdir = abs_outdir,\n",
        "  skip_grid = skip_grid,\n",
        "  skip_save = False,\n",
        "  strength = strength,\n",
        "  ddim_steps = ddim_steps,\n",
        "  sampler = sampler,\n",
        "  fixed_code = False,\n",
        "  ddim_eta = ddim_eta,\n",
        "  n_iter = abs_n_iter,\n",
        "  H = H,\n",
        "  W = W,\n",
        "  C = 4,\n",
        "  f = f,\n",
        "  n_samples = n_samples,\n",
        "  n_rows = 0,\n",
        "  scale = scale,\n",
        "  from_file = False,\n",
        "  seed = true_seed,\n",
        "  precision = precision,\n",
        "  recurse_img = None,\n",
        "  sub_strength = None\n",
        ")\n",
        "\n",
        "if use_optim_sd :\n",
        "  first_idx_img = opt_img2img(args) if use_img2img else opt_txt2img(args)\n",
        "else :\n",
        "  first_idx_img = img2img(args) if use_img2img else txt2img(args)\n",
        "\n",
        "print(f'Seed: {true_seed}')\n",
        "print(f'Images index: {list(range(first_idx_img, first_idx_img+n_samples*n_iter))}')\n",
        "\n",
        "settings_fname = f'{abs_outdir}/samples/{first_idx_img}-{-1+first_idx_img+n_samples*n_iter}_settings.txt'\n",
        "\n",
        "#save settings\n",
        "with open(settings_fname, 'w+') as f:\n",
        "    json.dump(vars(args), f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "fmafGmcyT1mZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clear GPU RAM function in case of unexpected CUDA out of memory error\n",
        "\n",
        "def refresh_cuda_memory():\n",
        "    \"\"\"\n",
        "    Re-allocate all cuda memory to help alleviate fragmentation\n",
        "    \"\"\"\n",
        "    # Run a full garbage collect first so any dangling tensors are released\n",
        "    gc.collect()\n",
        "\n",
        "    # Then move all tensors to the CPU\n",
        "    locations = {}\n",
        "    for obj in gc.get_objects():\n",
        "        try :\n",
        "          if not isinstance(obj, torch.Tensor):\n",
        "            continue\n",
        "        except : continue\n",
        "\n",
        "        locations[obj] = obj.device\n",
        "        obj.data = obj.data.cpu()\n",
        "        if isinstance(obj, torch.nn.Parameter) and obj.grad is not None:\n",
        "            obj.grad.data = obj.grad.cpu()\n",
        "\n",
        "    # Now empty the cache to flush the allocator\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Finally move the tensors back to their associated GPUs\n",
        "    for tensor, device in locations.items():\n",
        "        tensor.data = tensor.to(device)\n",
        "        if isinstance(tensor, torch.nn.Parameter) and tensor.grad is not None:\n",
        "            tensor.grad.data = tensor.grad.to(device)\n",
        "\n",
        "refresh_cuda_memory()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hq1nOo5Tzxzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run upscaler"
      ],
      "metadata": {
        "id": "SjODxHLTr37y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "upscale_factor = 4.0 #@param{type: \"number\"}\n",
        "downscale_factor = 1.0 #@param{type: \"number\"}\n",
        "upscale_enhance_faces = False #@param{type: \"boolean\"}\n",
        "results_idx = [431] #@param{type: \"raw\"}\n",
        "#@markdown `results_idx` must contain the list of the results (in the `outdir` folder) you want to upscale, eg. \\[0, 5, 6, 12\\].\n",
        "\n",
        "if load_upscaler :\n",
        "  for idx in results_idx :\n",
        "      fpath = f\"{abs_outdir}/samples\"\n",
        "      fname = upscale(fpath, f\"{idx}.png\", upscale_factor, downscale_factor, upscale_enhance_faces)\n",
        "      print(f\"{fpath}/{fname}\")\n",
        "else:\n",
        "  print(\"You haven't enabled the upscaler for this session. Please restart your session with 'load_upscaler' enabled.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rALkEnGEr8e8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}